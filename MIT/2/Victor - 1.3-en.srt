0
00:00:00,940 --> 00:00:02,996
In this segment, we have two learning goals.

1
00:00:02,996 --> 00:00:07,740
Goal 1 is to understand the analysis of variance, or ANOVA, in the population and

2
00:00:07,740 --> 00:00:08,840
in the sample.

3
00:00:08,840 --> 00:00:12,800
Goal 2 is to assess the out of sample predictive performance

4
00:00:12,800 --> 00:00:14,410
of the sample linear regression.

5
00:00:14,410 --> 00:00:16,530
We begin with the population case.

6
00:00:16,530 --> 00:00:20,370
Using the decomposition of Y into the explain and the residual part.

7
00:00:20,370 --> 00:00:24,070
And the normal equations that we derived in the previous segment,

8
00:00:24,070 --> 00:00:29,510
we can decompose variation of Y into the sum of explain variation and

9
00:00:29,510 --> 00:00:32,280
residual variation, as shown in the formula.

10
00:00:32,280 --> 00:00:35,330
We next define the population mean squared prediction error,

11
00:00:35,330 --> 00:00:38,850
or MSE, as the variance of the population residual.

12
00:00:38,850 --> 00:00:41,010
The expectation of epsilon squared.

13
00:00:41,010 --> 00:00:45,970
We also define the population R squared as the ratio of explained variation

14
00:00:45,970 --> 00:00:47,250
to the total variation.

15
00:00:47,250 --> 00:00:48,310
In other words,

16
00:00:48,310 --> 00:00:53,880
the population R squared is the variation of Y explained by the BLP.

17
00:00:53,880 --> 00:00:57,260
And as such, it is bounded below by 0 and above by 1.

18
00:00:57,260 --> 00:01:00,200
The analysis of variants in the sample process analogous.

19
00:01:00,200 --> 00:01:03,880
We simply replace population expectations by the empirical expectations.

20
00:01:03,880 --> 00:01:08,055
Using the decomposition of Yi to explain and the residual part, and

21
00:01:08,055 --> 00:01:10,710
the normal equations for the sample.

22
00:01:10,710 --> 00:01:15,850
We can decompose the sample variation of Yi into the sum of explained variation and

23
00:01:15,850 --> 00:01:16,780
residual variation.

24
00:01:16,780 --> 00:01:19,810
The former is given by the sample variation of this sample best in

25
00:01:19,810 --> 00:01:20,492
a predictor.

26
00:01:20,492 --> 00:01:23,760
And the latter is given by the sample variation of the residual.

27
00:01:23,760 --> 00:01:27,620
We can define the sample MSE as the sample variance of the residuals and

28
00:01:27,620 --> 00:01:30,560
we can define the sample R squared as the ratio

29
00:01:30,560 --> 00:01:32,820
of the X-plane to the total variation in the sample.

30
00:01:34,065 --> 00:01:36,005
We know that when P/N is small,

31
00:01:36,005 --> 00:01:38,981
the sample linear predictor gets really close to the best linear predictor.

32
00:01:38,981 --> 00:01:43,515
Thus when P/N is small, we expect that sample averages of y

33
00:01:43,515 --> 00:01:48,470
squared beta hat xi squared epsilon hat i squared to be close to

34
00:01:48,470 --> 00:01:53,160
the population averages of Y squared, beta X squared, and epsilon squared.

35
00:01:53,160 --> 00:01:56,960
So in this case, the sample R squared and the sample MSE

36
00:01:56,960 --> 00:02:01,458
will be close to the true quantities, the population R squared and MSE.

37
00:02:01,458 --> 00:02:03,640
When P / N is not small,

38
00:02:03,640 --> 00:02:07,230
the discrepancy between the two sets of measures can be substantial.

39
00:02:07,230 --> 00:02:08,550
And the sample R squared and

40
00:02:08,550 --> 00:02:12,120
the sample MSE can be very poor measures of predictability.

41
00:02:12,120 --> 00:02:16,413
For example, when p = n, we can have sample MSE = 0 and

42
00:02:16,413 --> 00:02:21,110
sample R squared = 1 no matter what the population MSE or R squared are.

43
00:02:22,300 --> 00:02:25,000
The following simulation example will support our reason.

44
00:02:25,000 --> 00:02:27,900
In this example, Y and X are statistically independent and

45
00:02:27,900 --> 00:02:32,120
generated from the normal distributions with mean need 0 and variance 1.

46
00:02:32,120 --> 00:02:36,120
The means that the true linear predictor of Y given X is simply 0 and

47
00:02:36,120 --> 00:02:38,055
the true R squared is also 0.

48
00:02:38,055 --> 00:02:41,538
Suppose the number of observations is N, the number of regressors is P.

49
00:02:41,538 --> 00:02:44,590
If p = n, then the typical sample R squared will be 1.

50
00:02:44,590 --> 00:02:47,060
Which is very far away from the true number of 0.

51
00:02:47,060 --> 00:02:48,882
This is an example of extreme over fitting.

52
00:02:48,882 --> 00:02:53,345
If p= n / 2 then the typical sample r squared is about half which is still

53
00:02:53,345 --> 00:02:55,170
far off from the truth.

54
00:02:55,170 --> 00:02:59,969
If p = n / 20, then the typical sample r squared is about 0.05 which is no

55
00:02:59,969 --> 00:03:02,190
longer far off from the truth.

56
00:03:02,190 --> 00:03:04,480
We can now see that using sample R squared and

57
00:03:04,480 --> 00:03:08,630
MSE to judge predicted performance could be misleading when P over N is not small.

58
00:03:08,630 --> 00:03:12,030
So the question is, can we design better metrics for predicted performance, and

59
00:03:12,030 --> 00:03:13,170
the answer is yes.

60
00:03:13,170 --> 00:03:16,088
The first proposal is to use the adjusted R squared and MSE.

61
00:03:16,088 --> 00:03:20,986
We first define the adjusted MSE by rescaling the sample MSE by a factor of n

62
00:03:20,986 --> 00:03:21,782
over n- p.

63
00:03:21,782 --> 00:03:24,880
And then making the corresponding adjustment to the sample R squared.

64
00:03:24,880 --> 00:03:26,580
The re-scaling will correct for over-fitting but

65
00:03:26,580 --> 00:03:28,620
under rather strong assumptions.

66
00:03:28,620 --> 00:03:32,480
A more universal way to measure predictive performance is to perform data splitting.

67
00:03:32,480 --> 00:03:36,330
First, we use a random part of data, say half of data, for estimating or

68
00:03:36,330 --> 00:03:37,800
training the prediction rules.

69
00:03:37,800 --> 00:03:41,690
Second, we use the other part of data to evaluate the predictive performance or

70
00:03:41,690 --> 00:03:45,670
the rule, recording the out-of-sample MSE or R squared.

71
00:03:45,670 --> 00:03:48,710
Accordingly we call the first part of data, the training sample.

72
00:03:48,710 --> 00:03:51,600
And the second part, the testing or validation sample.

73
00:03:51,600 --> 00:03:55,935
Indeed suppose we use n observations for training and m for testing or validation.

74
00:03:55,935 --> 00:04:00,300
Let capital V denote the names of observations in the test sample.

75
00:04:00,300 --> 00:04:04,315
Then the out of sample or test means squared error is defined as the average

76
00:04:04,315 --> 00:04:10,135
squared prediction error, where we predict yk in the test sample by head beta xk.

77
00:04:10,135 --> 00:04:12,780
Where hat beta was computed on the training sample.

78
00:04:12,780 --> 00:04:16,240
The out of sample R squared is defined accordingly as 1- the ratio

79
00:04:16,240 --> 00:04:19,900
of the test MSE to the variation of the outcome in the test sample.

80
00:04:19,900 --> 00:04:21,880
So let us summarize.

81
00:04:21,880 --> 00:04:26,040
First we discussed the analysis of variance, in population and in the sample.

82
00:04:27,080 --> 00:04:29,970
Second, we define good measures of predictive performance,

83
00:04:29,970 --> 00:04:34,320
based on adjusted MSE and R squared, and based on data splitting.

84
00:04:34,320 --> 00:04:37,150
And next we will discuss a real application to predicting wages.

