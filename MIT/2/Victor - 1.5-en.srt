0
00:00:00,840 --> 00:00:03,880
In this segment, we provide an answer to the inference question.

1
00:00:03,880 --> 00:00:07,440
To recall the question, we partition vector of regressors X into D and

2
00:00:07,440 --> 00:00:10,720
W, where D represents the target regressors of interest, and

3
00:00:10,720 --> 00:00:14,780
W represents other regressors, which we sometimes call the controls.

4
00:00:14,780 --> 00:00:22,395
We write Y = the predicted value which is beta 1D + beta 2W plus noise.

5
00:00:22,395 --> 00:00:24,460
And now we recall the inference question.

6
00:00:24,460 --> 00:00:28,640
Which is how does the predicted value of Y change if we increase D

7
00:00:28,640 --> 00:00:30,970
by a unit holding W fixed?

8
00:00:30,970 --> 00:00:31,700
For instance,

9
00:00:31,700 --> 00:00:35,550
in the wage example, the inference question can be stated as follows.

10
00:00:35,550 --> 00:00:38,080
What is the difference in the predicted wage for a man and

11
00:00:38,080 --> 00:00:40,970
woman with the same job-relevant characteristics?

12
00:00:40,970 --> 00:00:45,205
The answer to this question is the population regression coefficient beta 1

13
00:00:45,205 --> 00:00:47,215
corresponding to the target regressor D.

14
00:00:47,215 --> 00:00:50,185
In the wage example, D is the female indicator and

15
00:00:50,185 --> 00:00:52,325
beta 1 is called the gender wage gap.

16
00:00:52,325 --> 00:00:55,125
Next we attempt to understand better the meaning of beta 1

17
00:00:55,125 --> 00:00:57,250
using a tool called partialling-out.

18
00:00:57,250 --> 00:01:00,170
In the population, we define the partialling-out operation

19
00:01:00,170 --> 00:01:04,970
as a procedure that takes a random variable V and creates a residual to the V

20
00:01:04,970 --> 00:01:08,720
by subtracting the part of V that is linearly predicted by W.

21
00:01:08,720 --> 00:01:13,660
So we can write tilde V as V- gamma V where gamma indexed by

22
00:01:13,660 --> 00:01:17,400
vw solves the problem of best linear prediction of V using W.

23
00:01:17,400 --> 00:01:21,850
Here we use notation argument that does not solution of the minimization problem.

24
00:01:21,850 --> 00:01:25,690
It can be shown that the partialling-out operation is linear.

25
00:01:25,690 --> 00:01:31,470
So if we have Y = V + U, then tilde Y = tilde V + tilde U.

26
00:01:31,470 --> 00:01:34,950
We now apply partialling-out to both sides of our regression equation

27
00:01:34,950 --> 00:01:39,710
to get the partialled-out version which implies that we obtain the decomposition

28
00:01:39,710 --> 00:01:44,520
which reads tilde Y = beta 1 tilde D + epsilon,

29
00:01:44,520 --> 00:01:47,200
where epsilon is uncorrelated with tilde D.

30
00:01:47,200 --> 00:01:52,460
This decomposition follows because partialling out eliminates all the Ws,

31
00:01:52,460 --> 00:01:57,540
and also leaves epsilon untouched, since epsilon is linearly unpredictable by X,

32
00:01:57,540 --> 00:01:59,890
and therefore by W by definition.

33
00:01:59,890 --> 00:02:03,441
Moreover, since tilde D is a linear function of X, epsilon and

34
00:02:03,441 --> 00:02:06,440
tilde D are not correlated in our decomposition.

35
00:02:06,440 --> 00:02:09,775
Now recognize our decomposition as the normal equations for

36
00:02:09,775 --> 00:02:12,760
the population regression of tilde Y and tilde D.

37
00:02:12,760 --> 00:02:15,710
That is, we've just obtained the Frisch-Waugh-Lovell theorem,

38
00:02:15,710 --> 00:02:19,610
which states that the population linear regression coefficient beta 1 can be

39
00:02:19,610 --> 00:02:23,210
recovered from the population linear regression of tilde Y on tilde D.

40
00:02:23,210 --> 00:02:26,250
Beta 1 is a solution to the best linear prediction problem,

41
00:02:26,250 --> 00:02:28,860
where we predict tilde Y by a linear function of tilde D.

42
00:02:28,860 --> 00:02:30,930
We can also give an explicit formula for beta 1,

43
00:02:30,930 --> 00:02:33,960
which is given by the ratio of two averages that you see in this formula.

44
00:02:35,130 --> 00:02:37,740
We also see that beta 1 is uniquely defined

45
00:02:37,740 --> 00:02:40,040
if D is not perfectly predicted by the Ws.

46
00:02:40,040 --> 00:02:42,168
So the tilde D has a non-zero variance.

47
00:02:42,168 --> 00:02:45,800
The Frisch-Waugh-Lovell theorem is a remarkable fact which is useful for

48
00:02:45,800 --> 00:02:47,560
both interpretation and estimation.

49
00:02:47,560 --> 00:02:51,280
It asserts that beta 1 can be interpreted as a regression coefficient

50
00:02:51,280 --> 00:02:55,035
of residualized Y on residualized D, where the residuals are defined by taking out or

51
00:02:55,035 --> 00:02:59,060
partialling-out the linear effect of W from Y and D.

52
00:02:59,060 --> 00:03:01,370
We next proceed to discuss estimation of beta 1.

53
00:03:01,370 --> 00:03:04,730
In the sample, we will mimic the partialling-out in the population.

54
00:03:04,730 --> 00:03:08,230
When p over n is small we can rely on the sample linear regression,

55
00:03:08,230 --> 00:03:10,050
that is on ordinary in these squares.

56
00:03:10,050 --> 00:03:13,245
When p over n is not small using sample linear regression for

57
00:03:13,245 --> 00:03:14,800
partialling-out is not a good idea.

58
00:03:14,800 --> 00:03:17,180
What we can do instead is do penalized regression or

59
00:03:17,180 --> 00:03:20,350
variables selection, which we will discuss later in this module.

60
00:03:20,350 --> 00:03:22,260
So let us assume that p over n is small.

61
00:03:22,260 --> 00:03:25,590
So it is appropriate to use the sample linear regression for partialling-out.

62
00:03:25,590 --> 00:03:28,640
Of course, by the Frisch–Waugh–Lovell theorem applied to the sample instead of

63
00:03:28,640 --> 00:03:31,640
the population, the sample linear regression of Y on D and

64
00:03:31,640 --> 00:03:35,680
W gives us an estimator hat beta 1, which is numerically equivalent

65
00:03:35,680 --> 00:03:38,030
to the estimator obtained via sample partialling-out.

66
00:03:38,030 --> 00:03:41,550
It is still useful to give the formula for hat beta 1 in terms of

67
00:03:41,550 --> 00:03:46,060
sample partialling-out where we use checked quantities to denote the residuals

68
00:03:46,060 --> 00:03:49,880
that are left after predicting the variables in the sample with the controls.

69
00:03:49,880 --> 00:03:53,386
The population partialling-out is replaced here by the sample partialling-out

70
00:03:53,386 --> 00:03:56,930
where V replace the population expectation by the empirical expectation.

71
00:03:56,930 --> 00:04:00,410
Using the formula, it can be shown that the following results calls.

72
00:04:00,410 --> 00:04:03,740
If p over n is small, then the estimation error in the estimated

73
00:04:03,740 --> 00:04:07,320
residualized quantities has a negligible effect on hat beta 1, and

74
00:04:07,320 --> 00:04:12,130
hat beta 1 is approximately distributed as normal variable with mean beta 1 and

75
00:04:12,130 --> 00:04:16,680
variance V over n where the expression of the variance appears over here.

76
00:04:16,680 --> 00:04:20,800
In words, we can say that the estimator hat beta 1 concentrates in a square root

77
00:04:20,800 --> 00:04:25,030
of V over n neighborhood of beta 1 with deviations controlled by the normal law.

78
00:04:25,030 --> 00:04:27,000
We can now define the standard error for

79
00:04:27,000 --> 00:04:32,320
hat beta 1 as square root of hat V over n, where hat V is an estimator of V.

80
00:04:32,320 --> 00:04:36,530
This result implies that the interval given by the estimate +- 2 standard

81
00:04:36,530 --> 00:04:40,590
errors covers the true value of beta 1, for most realizations of the data sample.

82
00:04:40,590 --> 00:04:44,930
Or more precisely approximately 95% of realizations of the data sample.

83
00:04:44,930 --> 00:04:48,440
If we replace 2 by other constants, we get other coverage probabilities.

84
00:04:48,440 --> 00:04:49,120
In other words,

85
00:04:49,120 --> 00:04:52,640
if our data sample is not extremely unusual, the interval covers the truth.

86
00:04:52,640 --> 00:04:55,380
For this reason, this interval is called the confidence interval.

87
00:04:55,380 --> 00:04:58,080
For example, in the wage example,

88
00:04:58,080 --> 00:05:02,400
our estimate of gender hourly wage gap is about -2$ and

89
00:05:02,400 --> 00:05:07,026
then 95% confidence interval is about -1$ to -3$.

90
00:05:07,026 --> 00:05:11,570
Let us summarize, first we interpreted beta 1 as the regression

91
00:05:11,570 --> 00:05:15,130
coefficient in the univariate regression of the response variable and

92
00:05:15,130 --> 00:05:19,030
the target variable, after we have removed the linear effect of other variables.

93
00:05:19,030 --> 00:05:22,398
Second, we noted that this result is useful for interpretation and

94
00:05:22,398 --> 00:05:25,160
understanding of the regression coefficient.

95
00:05:25,160 --> 00:05:28,465
This result will also be super useful for setting up inference in modern

96
00:05:28,465 --> 00:05:31,650
high-dimensional settings which we will discussed later in this module.

97
00:05:31,650 --> 00:05:34,830
And next, we will carry out a case study for our wage example.

