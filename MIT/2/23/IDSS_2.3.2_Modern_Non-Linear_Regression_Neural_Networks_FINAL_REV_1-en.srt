0
00:00:00,860 --> 00:00:01,620
In this segment,

1
00:00:01,620 --> 00:00:05,320
we consider a non linear regression method, based on neural networks.

2
00:00:05,320 --> 00:00:11,180
As before, we are interested in predicting the outcome Y using the raw regressors Z,

3
00:00:11,180 --> 00:00:12,500
what are K-dimensional.

4
00:00:12,500 --> 00:00:17,590
Recall that the best prediction rule of Y given Z is the function g(Z),

5
00:00:17,590 --> 00:00:20,030
the conditional expectation of Y given Z.

6
00:00:20,030 --> 00:00:24,370
The idea of the neural network is to use parameterized nonlinear transformations of

7
00:00:24,370 --> 00:00:28,830
linear combinations of the raw regressors as constructed regressors, called neurons.

8
00:00:28,830 --> 00:00:33,380
And produce the predicted value as a linear function of these regressors.

9
00:00:33,380 --> 00:00:38,280
The prediction rule is g(Z) is nonlinear in some parameters and

10
00:00:38,280 --> 00:00:40,340
with respect to raw regressors.

11
00:00:40,340 --> 00:00:42,113
With sufficient many neurons neurons,

12
00:00:42,113 --> 00:00:46,440
g(Z) can approximate the best prediction rule g(Z).

13
00:00:46,440 --> 00:00:50,770
In part 2 of our module, we already saw that many constructed regressors

14
00:00:50,770 --> 00:00:55,390
are useful in the high-dimensional linear setting to approximate g(Z).

15
00:00:55,390 --> 00:00:58,785
Neural networks also rely on many constructed regressors to approximate

16
00:00:58,785 --> 00:00:59,410
g(Z).

17
00:00:59,410 --> 00:01:02,930
The method and the name neural networks were loosely inspired by the mode of

18
00:01:02,930 --> 00:01:04,500
operation of the human brain, and

19
00:01:04,500 --> 00:01:07,970
developed by the scientists working on Artificial Intelligence.

20
00:01:07,970 --> 00:01:11,290
Accordingly, neural networks can be represented by cool graphs and

21
00:01:11,290 --> 00:01:14,160
diagrams that we will discuss shortly, so please stay tuned.

22
00:01:14,160 --> 00:01:18,760
To discuss the idea in more detail, we focus on the single layer neural network.

23
00:01:18,760 --> 00:01:25,041
The estimated prediction rule will take the form hat g(Z) equals sum over M,

24
00:01:25,041 --> 00:01:30,450
running from 1 to capital M of hat betaM times Xm of hat alphaM.

25
00:01:30,450 --> 00:01:35,980
Where the Xm of hat alpha m's are constructed regressors called neurons.

26
00:01:35,980 --> 00:01:37,850
The capital M neurons in total.

27
00:01:37,850 --> 00:01:43,830
The neurons are generated by the formula Xm of alpha m = sigma of alpha mZ.

28
00:01:43,830 --> 00:01:48,540
Where alpha m's are neuron-specific vectors of parameters called weights and

29
00:01:48,540 --> 00:01:50,910
sigma is the activation function.

30
00:01:50,910 --> 00:01:54,485
For example, sigma can be the sigmoid function given by this formula.

31
00:01:54,485 --> 00:01:55,340
Or it can be the so

32
00:01:55,340 --> 00:02:00,530
called rectified linear unit function or ReLU given by this formula.

33
00:02:00,530 --> 00:02:03,835
The following figure shows the two graphs of the two Activation Functions.

34
00:02:03,835 --> 00:02:06,864
The sigmoid function and the rectified linear unit function.

35
00:02:06,864 --> 00:02:09,664
The horizontal axis shows the value of the argument.

36
00:02:09,664 --> 00:02:12,055
And the vertical axis the value of the function.

37
00:02:12,055 --> 00:02:16,714
The estimators had alpha m, and beta m for each M are obtained as the solution to

38
00:02:16,714 --> 00:02:21,540
the penalized nonlinear least squares problem shown by this formula.

39
00:02:21,540 --> 00:02:25,890
Here, we are minimizing the sum of squared prediction erros in the sample,

40
00:02:25,890 --> 00:02:29,010
plus a penalty term given by the sum of the absolute values

41
00:02:29,010 --> 00:02:33,530
of components of alpha m and beta m, multiplied by the penalty level, lambda.

42
00:02:33,530 --> 00:02:35,930
In this formula, we use the lasso type penalty but

43
00:02:35,930 --> 00:02:38,800
we can also use the ridge, another type of penalties.

44
00:02:38,800 --> 00:02:42,700
The estimates are computed using sophisticated gradient descent algorithms.

45
00:02:42,700 --> 00:02:45,905
Where sophistication is needed because nonlinear least squares optimization

46
00:02:45,905 --> 00:02:47,870
problem is generally not a convex problem.

47
00:02:47,870 --> 00:02:49,870
Making the computation a difficult task.

48
00:02:49,870 --> 00:02:53,270
The procedural fetching renewal network model has tuning parameters and

49
00:02:53,270 --> 00:02:55,570
in practice we can choose them by cross-validation.

50
00:02:55,570 --> 00:02:58,270
The most important choices concern the number of neurons and

51
00:02:58,270 --> 00:02:59,780
the number of neuron layers.

52
00:02:59,780 --> 00:03:01,980
Having more neurons gives us more flexibility,

53
00:03:01,980 --> 00:03:05,220
just like having more constructed regressors gave us more flexibility

54
00:03:05,220 --> 00:03:07,080
with high-dimensional linear models.

55
00:03:07,080 --> 00:03:08,350
To prevent overfitting,

56
00:03:08,350 --> 00:03:11,620
we can rely on penalization as in the case of linear regression.

57
00:03:11,620 --> 00:03:13,710
In order to visualize working of the neural network,

58
00:03:13,710 --> 00:03:17,450
we rely on the resource called Playground.TensorFlow.org using which we

59
00:03:17,450 --> 00:03:22,160
produce a prediction model given by simple single layer neural network model.

60
00:03:22,160 --> 00:03:24,770
We now see the graphical representation of this network.

61
00:03:24,770 --> 00:03:28,085
Here we have a regression problem and the network depicts the process of taking

62
00:03:28,085 --> 00:03:31,090
row regressors and transforming them into predicted values.

63
00:03:31,090 --> 00:03:34,010
In the second column on the left, we see the inputs are features.

64
00:03:34,010 --> 00:03:36,570
These features are our two row regressors.

65
00:03:36,570 --> 00:03:38,830
The third column shows eight neurons.

66
00:03:38,830 --> 00:03:41,360
The neurons are constructed as linear combinations of the row

67
00:03:41,360 --> 00:03:43,870
regressors transformed by an activation function.

68
00:03:43,870 --> 00:03:47,800
That is, the neurons are along linear transformations of the row regressors.

69
00:03:47,800 --> 00:03:51,568
Here, we set the activation function, to be the rectified linear unit function,

70
00:03:51,568 --> 00:03:53,250
RE of U.

71
00:03:53,250 --> 00:03:55,230
The neurons are connected to the inputs and

72
00:03:55,230 --> 00:03:59,630
the connections represent the coefficients hat alpha M, which are coefficients

73
00:03:59,630 --> 00:04:02,770
of the neuron specific linear transformations of raw regressors.

74
00:04:02,770 --> 00:04:06,045
The coloring represents the sign or the coefficients, orange negative and

75
00:04:06,045 --> 00:04:06,925
the blue positive.

76
00:04:06,925 --> 00:04:10,420
And with all the connections represents the size of the coefficients.

77
00:04:10,420 --> 00:04:14,510
The neurons are then linearly combined to produce the output, the prediction rule.

78
00:04:14,510 --> 00:04:17,450
In the diagram we see the connections going outwards

79
00:04:17,450 --> 00:04:19,590
from the neurons to the output.

80
00:04:19,590 --> 00:04:22,700
These connections represent the coefficients hat beta M, or

81
00:04:22,700 --> 00:04:26,680
the linear combination of the neurons that produce the final output.

82
00:04:26,680 --> 00:04:30,380
The coloring and the widths represent the sign and the size of these coefficients.

83
00:04:30,380 --> 00:04:31,170
In the diagram,

84
00:04:31,170 --> 00:04:34,610
the prediction rule is shown by the heatmap in the box on the right.

85
00:04:34,610 --> 00:04:38,530
On the horizontal and vertical axis, we see the values of the two inputs.

86
00:04:38,530 --> 00:04:42,330
The color and its intensity in the heatmap represent the predicted value.

87
00:04:42,330 --> 00:04:45,650
We also see on the top that the penalty function is L1, which stands for

88
00:04:45,650 --> 00:04:46,970
the lasso type penalty.

89
00:04:46,970 --> 00:04:50,260
Another option is to use L2, which stands for the rich type penalty.

90
00:04:50,260 --> 00:04:53,440
The penalty level is called here the regularization rate.

91
00:04:53,440 --> 00:04:56,750
In this example we are using a single layer neural network.

92
00:04:56,750 --> 00:04:57,860
If we add one or

93
00:04:57,860 --> 00:05:01,010
two additional layers of neurons constructed from the previous layer of

94
00:05:01,010 --> 00:05:05,180
neurons, we get a different network, which we show in the following diagram.

95
00:05:05,180 --> 00:05:09,350
Prediction methods based on neural networks with several layers of neurons

96
00:05:09,350 --> 00:05:10,920
called the deep learning methods.

97
00:05:10,920 --> 00:05:14,460
This diagrams showcases that one of the major benefits of doing prediction with

98
00:05:14,460 --> 00:05:17,670
neural networks is that you can adapt with pretty cool artwork.

99
00:05:17,670 --> 00:05:21,200
Let us summarize, in this segment we have discussed neural networks that have

100
00:05:21,200 --> 00:05:24,560
recently gathered much attention under the umbrella of deep learning.

101
00:05:24,560 --> 00:05:27,600
Neural networks represent a powerful and all-purpose method for

102
00:05:27,600 --> 00:05:29,860
prediction and regression analysis.

103
00:05:29,860 --> 00:05:33,480
Using many neurons and multiple layers gives rise to networks that are very

104
00:05:33,480 --> 00:05:36,860
flexible and can approximate the best prediction rules quiet well.

